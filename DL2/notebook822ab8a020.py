# -*- coding: utf-8 -*-
"""notebook822ab8a020

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/notebook822ab8a020-3706dde8-c800-477f-90b0-70594336de08.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240908/auto/storage/goog4_request%26X-Goog-Date%3D20240908T132421Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D3cef12d97e307e6c5de2c482c973eaa8f272c70505df31c8661e7898461c804d656afc1981bf90c54377d26519d965341c96736407e65c6ff7f81a6b00f25956dc63db04269f10c20b3ac14e429c253970b1ce8a4130669d59608d37eda76a61fb2ce9d5a8a24bdc288ee3177c331812eb739cdc55ac26fb1f7f5b39a856cfdbdbd9c0bee804bacd326d172e4cf053b87456553a9bf800425947ded26b49fe0da9e369854c131f2861a0e520dcb8e16802c4c4c1951fc90dcf79e3a81ae2285215e67e52a9aa6a815e982278cf7a3b2ae36fd6ae8f82766cb1d8a53a058a9a8f14185f717dfd2d1ba9efdb0285d49a0ed0c7d5d99d05021e343f68c5f84d70d8
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'medium-articles:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F2123951%2F3531703%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240908%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240908T132421Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Dbde8da532b6f969870a7bfe120aeea8df200c0a95703531b71e6768d3a556bb5d8e6af4eb3df1d9ee11ef1f9b45b7ff51cba1ee694cf1076f05d3cb3216a5f64336265c48fade5f49382ea84dc7b22c7773bb44a84255e79fd7bf63adeb20ab2138a9a4c03da0cf90deb9143a80b92d799ae4338f516cff84d442049fc6ffb1fd5e286124c84c7943bf7e4e937c341ada48bb7166ceae2ca2456a7faa665788213957b7bca87e7cb6d65f6531a82b04cf835d74694e7bd529b5b84c04c53463e2d371bfaa9fceae7352b170fc8f5b3c4de407ee10b09792dd0473b61bbd7c7bf1a46bf40adf10d1caabd5d31d073a8aa5a5af99c607a1820ee23f40030469491'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

"""# New Section"""

import numpy as np
import pandas as pd
import nltk
import re
from nltk.tokenize import sent_tokenize
from nltk.corpus import stopwords
from gensim.models import Word2Vec
from scipy import spatial
import networkx as nx
import csv
from scipy.spatial import distance


nltk.download('punkt')
nltk.download('stopwords')

df=pd.read_csv('/kaggle/input/medium-articles/medium_articles.csv')
df.head()

df = df[['title', 'text']]
df.shape

df = df.drop_duplicates()
df.shape

df["text"] = df["text"].str.replace('\n', '</n/>')

sample_blog = df["text"][10]
print(sample_blog)

sentences = sent_tokenize(sample_blog)

sentences_clean = [re.sub(r'[^\w\s]', '', sentence.lower()) for sentence in sentences]

stop_words = stopwords.words('english')
sentence_tokens = [[word for word in sentence.split() if word not in stop_words] for sentence in sentences_clean]

w2v = Word2Vec(sentence_tokens, vector_size=100, min_count=1, epochs=1000)

sentence_embeddings = [np.mean([w2v.wv.get_vector(word) for word in words], axis=0) if words else np.zeros(w2v.vector_size) for words in sentence_tokens]

similarity_matrix = np.zeros([len(sentence_tokens), len(sentence_tokens)])

for i, row_embedding in enumerate(sentence_embeddings):
    for j, column_embedding in enumerate(sentence_embeddings):
        similarity_matrix[i][j] = 1 - distance.cosine(row_embedding, column_embedding)

threshold = 0.1
similarity_matrix[similarity_matrix < threshold] = 0

nx_graph = nx.from_numpy_array(similarity_matrix)

try:
    scores = nx.pagerank(nx_graph, max_iter=1000, alpha=0.75)
except nx.PowerIterationFailedConvergence as e:
    print(f"PageRank failed to converge: {e}")

top_sentence = {sentence: scores[index] for index, sentence in enumerate(sentences)}

sentNeeded = round(0.25 * len(sentences)) - 1
top = dict(sorted(top_sentence.items(), key=lambda x: x[1], reverse=True)[:sentNeeded])

summary = ""
for sent in sentences:
    if sent in top.keys():
        summary += sent

print(summary)

count = 0
def generateSummary(blog):
  global count
  count = 1
  print("Summarising blog, count")
  try:
    sentences=sent_tokenize(blog)
    sentences_clean=[re.sub(r'[^\w\s]','', sentence.lower()) for sentence in sentences]
    stop_words=stopwords.words('english')
    sentence_tokens=[[words for words in sentence.split(' ') if words not in stop_words] for sentence in sentences_clean]
    w2v=Word2Vec(sentence_tokens, vector_size=1,min_count=1, epochs=1000)
    sentence_embeddings=[[w2v.wv.get_vector(word) [8] for word in words] for words in sentence_tokens]
    max_len=max([len (tokens) for tokens in sentence_tokens ])
    sentence_embeddings=[np.pad(embedding, (0, max_len-len(embedding)), 'constant') for embedding in sentence_embeddings]
    similarity_matrix=np.zeros([len(sentence_tokens), len(sentence_tokens)])
    for i, row_embedding in enumerate(sentence_embeddings):
      for j,column_embedding in enumerate(sentence_embeddings):
        similarity_matrix[i][j]=1-spatial.distance.cosine(row_embedding,column_embedding)

    nx_graph=nx.from_numpy_array(similarity_matrix)
    scores=nx.pagerank(nx_graph, max_iter=600)
    top_sentence={sentence:scores [index] for index, sentence in enumerate (sentences)}
    sentNeeded=round(0.25*len(sentences)) - 1
    top = dict(sorted(top_sentence.items(), key=lambda x: x[1], reverse=True)[:sentNeeded])
    summary = ''
    for sent in sentences:
      if sent in top.keys():
          summary+=sent
    return summary
  except:
    return float("NaN")

import math
filename="articlesSet.csv"
fields=['title", "summary', 'content']

with open(filename, 'a') as csvfile:
  csvwriter = csv.writer(csvfile)
  csvwriter.writerow(fields)

  def callback(row):
    summary=generateSummary(row['text'])
    if(type(summary)!=str):
        return
    rows = [row['title'], summary, row['text']]
    csvwriter.writerow(rows)

  df.apply(callback, axis=1)